{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 19: Multiple Regression and Model Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Making Predictions in Statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('heightWeight.csv')\n",
    "plt.scatter(df.height, df.weight)\n",
    "plt.xlabel(\"Height\")\n",
    "plt.ylabel(\"Weight\")\n",
    "plt.title(\"Scatterplot of Height vs Weight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'weight~height'\n",
    "model = ols(formula=f, data=df).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(df.iloc[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_predict = 4\n",
    "\n",
    "print('Actual weight: ', df.iloc[to_predict]['weight'])\n",
    "print('Predicted weight: ', model.predict(df.iloc[to_predict])[to_predict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_weight(height):\n",
    "    w = -204.4834 + 5.539*height\n",
    "    return f\"With a height of {height} inches, predicted weight of {round(w, 2)} pounds.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_weight(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Multiple Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_df = pd.read_csv('http://faculty.marshall.usc.edu/gareth-james/ISL/Auto.csv', na_values='?')\n",
    "car_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = 'mpg ~ cylinders+displacement+acceleration+weight+year+origin'\n",
    "model = ols(formula=formula, data=car_df).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(16,3))\n",
    "\n",
    "for xcol, ax in zip(['acceleration', 'displacement', 'horsepower', 'weight'], axes):\n",
    "    car_df.plot(kind='scatter', x=xcol, y='mpg', ax=ax, alpha=0.4, color='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12,3))\n",
    "\n",
    "for xcol, ax in zip([ 'cylinders', 'year', 'origin'], axes):\n",
    "    car_df.plot(kind='scatter', x=xcol, y='mpg', ax=ax, alpha=0.4, color='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyl_dummies = pd.get_dummies(car_df['cylinders'], prefix='cyl', drop_first=True)\n",
    "yr_dummies = pd.get_dummies(car_df['year'], prefix='yr', drop_first=True)\n",
    "orig_dummies = pd.get_dummies(car_df['origin'], prefix='orig', drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = car_df.drop(['cylinders','year','origin', 'name'], axis=1)\n",
    "data = pd.concat([data, cyl_dummies, yr_dummies, orig_dummies], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = 'mpg ~ '+ '+'.join(data.columns[1:])\n",
    "formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ols(formula=formula, data=data).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d. Multicollinearity of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first check for linearity between target and features\n",
    "data.corr()[abs(data.corr()['mpg']) > 0.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set1 = list(data.corr()[abs(data.corr()['mpg']) > 0.3].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.loc[:,set1]\n",
    "data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['weight', 'cyl_4'], axis=1)\n",
    "data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data.drop(['displacement'], axis=1)\n",
    "data2.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data2.drop(['cyl_8'], axis=1)\n",
    "data2.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = 'mpg ~ '+ '+'.join(data2.columns[1:])\n",
    "formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ols(formula=formula, data=data).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e. Transforming Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_df[['acceleration', 'displacement', 'horsepower', 'weight']].hist(figsize  = [6, 6])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data_log = pd.DataFrame([])\n",
    "data_log['logdisp'] = np.log(car_df['displacement'])\n",
    "data_log['loghorse'] = np.log(car_df['horsepower'])\n",
    "data_log['logweight'] = np.log(car_df['weight'])\n",
    "data_log.hist(figsize  = [6, 6]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log transformation\n",
    "\n",
    "As seen in the previous lesson, a log transformation is a very useful tool when you have data that clearly does not follow a normal distribution. Log transformation can help reduce skewness when you have skewed data, and can help reducing variability of data. \n",
    "\n",
    "\n",
    "### Min-max scaling\n",
    "\n",
    "When performing min-max scaling, you can transform x to get the transformed $x'$ by using the formula:\n",
    "\n",
    "$$x' = \\dfrac{x - \\min(x)}{\\max(x)-\\min(x)}$$\n",
    "\n",
    "This way of scaling brings all values between 0 and 1. \n",
    "\n",
    "### Standardization\n",
    "\n",
    "When \n",
    "\n",
    "$$x' = \\dfrac{x - \\bar x}{\\sigma}$$\n",
    "\n",
    "x' will have mean $\\mu = 0$ and $\\sigma = 1$\n",
    "\n",
    "Note that standardization does not make data $more$ normal, it will just change the mean and the standard error!\n",
    "\n",
    "### Mean normalization\n",
    "When performing mean normalization, you use the following formula:\n",
    "$$x' = \\dfrac{x - \\text{mean}(x)}{\\max(x)-\\min(x)}$$\n",
    "\n",
    "The distribution will have values between -1 and 1, and a mean of 0.\n",
    "\n",
    "### Unit vector transformation\n",
    " When performing unit vector transformations, you can create a new variable x' with a range [0,1]:\n",
    " \n",
    "$$x'= \\dfrac{x}{{||x||}}$$\n",
    "\n",
    "\n",
    "Recall that the norm of x $||x||= \\sqrt{(x_1^2+x_2^2+...+x_n^2)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = car_df['acceleration']\n",
    "logdisp = data_log['logdisp']\n",
    "loghorse = data_log['loghorse']\n",
    "logweight = data_log['logweight']\n",
    "\n",
    "scaled_acc = (acc - min(acc)) / (max(acc) - min(acc))\n",
    "scaled_disp = (logdisp - np.mean(logdisp)) / np.sqrt(np.var(logdisp))\n",
    "scaled_weight = (logweight - np.mean(logweight)) / np.sqrt(np.var(logweight))\n",
    "scaled_horse = (loghorse - np.mean(loghorse)) / (max(loghorse) - min(loghorse))\n",
    "\n",
    "data_cont_scaled = pd.DataFrame([])\n",
    "data_cont_scaled['acc'] = scaled_acc\n",
    "data_cont_scaled['disp'] = scaled_disp\n",
    "data_cont_scaled['horse'] = scaled_horse\n",
    "data_cont_scaled['weight'] = scaled_weight\n",
    "\n",
    "data_cont_scaled.hist(figsize = [6, 6]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f. Model Validation & Regression in Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The need for train-test split\n",
    "\n",
    "#### Making predictions and evaluation\n",
    "\n",
    "So far we've simply been fitting models to data, and evaluated our models calculating the errors between our $\\hat y$ and our actual targets $y$, while these targets $y$ contributed in fitting the model.\n",
    "\n",
    "The reason why we built the model in the first place, however, is because we want to predict the outcome for observations that are not necessarily in our dataset now; e.g: we want to predict miles per gallon for a new car that isn't part of our dataset, or for a new house in Boston.\n",
    "\n",
    "In order to get a good sense of how well your model will be doing on new instances, you'll have to perform a so-called \"train-test-split\". What you'll be doing here, is take a sample of the data that serves as input to \"train\" our model - fit a linear regression and compute the parameter estimates for our variables, and calculate how well our predictive performance is doing comparing the actual targets $y$ and the fitted $\\hat y$ obtained by our model.\n",
    "\n",
    "#### Underfitting and overfitting\n",
    "\n",
    "Another reason to use train-test-split is because of a common problem which doesn't only affect linear models, but nearly all (other) machine learning algorithms: overfitting and underfitting. An overfit model is not generalizable and will not hold to future cases. An underfit model does not make full use of the information available and produces weaker predictions than is feasible. The following image gives a nice, more general demonstration:\n",
    "\n",
    "<img src=\"images/modelfit.png\" width=\"700\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is pretty straightforward that, to evaluate the model, you'll want to compare your predicted values, $\\hat y$ with the actual value, $y$. The difference between the two values is referred to as the residuals. When using a train-test split, you'll compare your residuals for both test set and training set:\n",
    "\n",
    "$r_{i,train} = y_{i,train} - \\hat y_{i,train}$ \n",
    "\n",
    "$r_{i,test} = y_{i,test} - \\hat y_{i,test}$ \n",
    "\n",
    "To get a summarized measure over all the instances in the test set and training set, a popular metric is the (Root) Mean Squared Error:\n",
    "\n",
    "RMSE = $\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat y_{i})^2}$\n",
    "\n",
    "MSE = $\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - \\hat y_{i})^2$\n",
    "\n",
    "Again, you can compute these for both the traing and the test set. A big difference in value between the test and training set (R)MSE is an indication of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('http://faculty.marshall.usc.edu/gareth-james/ISL/Auto.csv', na_values='?') \n",
    "\n",
    "acc = data['acceleration']\n",
    "logdisp = np.log(data['displacement'])\n",
    "loghorse = np.log(data['horsepower'])\n",
    "logweight = np.log(data['weight'])\n",
    "\n",
    "scaled_acc = (acc-min(acc))/(max(acc)-min(acc))\n",
    "scaled_disp = (logdisp-np.mean(logdisp))/np.sqrt(np.var(logdisp))\n",
    "scaled_horse = (loghorse-np.mean(loghorse))/(max(loghorse)-min(loghorse))\n",
    "scaled_weight = (logweight-np.mean(logweight))/np.sqrt(np.var(logweight))\n",
    "\n",
    "data_fin = pd.DataFrame([])\n",
    "data_fin['acc'] = scaled_acc\n",
    "data_fin['disp'] = scaled_disp\n",
    "data_fin['horse'] = scaled_horse\n",
    "data_fin['weight'] = scaled_weight\n",
    "cyl_dummies = pd.get_dummies(data['cylinders'], prefix='cyl', drop_first=True)\n",
    "yr_dummies = pd.get_dummies(data['year'], prefix='yr', drop_first=True)\n",
    "orig_dummies = pd.get_dummies(data['origin'], prefix='orig', drop_first=True)\n",
    "mpg = data['mpg']\n",
    "data_fin = pd.concat([mpg, data_fin, cyl_dummies, yr_dummies, orig_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-Learn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([mpg, scaled_acc, scaled_weight, orig_dummies], axis=1)\n",
    "y = data[['mpg']]\n",
    "X = data.drop(['mpg'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train, y_train)\n",
    "\n",
    "y_hat_train = linreg.predict(X_train)\n",
    "y_hat_test = linreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "train_mse = mean_squared_error(y_train, y_hat_train)\n",
    "test_mse = mean_squared_error(y_test, y_hat_test)\n",
    "print('Train Mean Squarred Error:', train_mse)\n",
    "print('Test Mean Squarred Error:', test_mse)\n",
    "\n",
    "print('Train Root Mean Squarred Error:', train_mse**0.5)\n",
    "print('Test Root Mean Squarred Error:', test_mse**0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
